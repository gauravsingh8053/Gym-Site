
==> Audit <==
|------------|-----------------------|----------|-------|---------|---------------------|---------------------|
|  Command   |         Args          | Profile  | User  | Version |     Start Time      |      End Time       |
|------------|-----------------------|----------|-------|---------|---------------------|---------------------|
| start      | --driver=docker       | minikube | aryan | v1.35.0 | 06 May 25 00:05 PDT |                     |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 00:19 PDT |                     |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 00:29 PDT |                     |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 00:40 PDT |                     |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 00:41 PDT |                     |
| stop       |                       | minikube | aryan | v1.35.0 | 06 May 25 01:31 PDT | 06 May 25 01:31 PDT |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 01:34 PDT | 06 May 25 01:35 PDT |
| ip         |                       | minikube | aryan | v1.35.0 | 06 May 25 02:16 PDT | 06 May 25 02:16 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:19 PDT | 06 May 25 02:19 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 02:19 PDT | 06 May 25 02:19 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 02:24 PDT | 06 May 25 02:24 PDT |
| ssh        |                       | minikube | aryan | v1.35.0 | 06 May 25 02:26 PDT | 06 May 25 02:26 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 02:27 PDT | 06 May 25 02:27 PDT |
| stop       |                       | minikube | aryan | v1.35.0 | 06 May 25 02:28 PDT | 06 May 25 02:29 PDT |
| delete     |                       | minikube | aryan | v1.35.0 | 06 May 25 02:29 PDT | 06 May 25 02:29 PDT |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 02:32 PDT |                     |
| start      |                       | minikube | aryan | v1.35.0 | 06 May 25 02:34 PDT | 06 May 25 02:37 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:37 PDT | 06 May 25 02:38 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:38 PDT | 06 May 25 02:38 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 02:39 PDT | 06 May 25 02:39 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:41 PDT | 06 May 25 02:41 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:41 PDT | 06 May 25 02:42 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 02:43 PDT | 06 May 25 02:43 PDT |
| image      | load mywebsite:latest | minikube | aryan | v1.35.0 | 06 May 25 02:43 PDT | 06 May 25 02:44 PDT |
| stop       |                       | minikube | aryan | v1.35.0 | 06 May 25 02:44 PDT | 06 May 25 02:44 PDT |
| start      | --driver=docker       | minikube | aryan | v1.35.0 | 06 May 25 03:41 PDT | 06 May 25 03:42 PDT |
| docker-env |                       | minikube | aryan | v1.35.0 | 06 May 25 03:43 PDT | 06 May 25 03:43 PDT |
| service    | website-service       | minikube | aryan | v1.35.0 | 06 May 25 03:44 PDT |                     |
| image      | load my-website-image | minikube | aryan | v1.35.0 | 06 May 25 03:51 PDT |                     |
| image      | load my-website-image | minikube | aryan | v1.35.0 | 06 May 25 03:52 PDT |                     |
| service    | website-service       | minikube | aryan | v1.35.0 | 06 May 25 04:00 PDT |                     |
|------------|-----------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/06 03:41:21
Running on machine: DESKTOP-H4IMVF3
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0506 03:41:21.997644   40539 out.go:345] Setting OutFile to fd 1 ...
I0506 03:41:22.000842   40539 out.go:397] isatty.IsTerminal(1) = true
I0506 03:41:22.000881   40539 out.go:358] Setting ErrFile to fd 2...
I0506 03:41:22.000926   40539 out.go:397] isatty.IsTerminal(2) = true
I0506 03:41:22.002122   40539 root.go:338] Updating PATH: /home/aryan/.minikube/bin
I0506 03:41:22.010210   40539 out.go:352] Setting JSON to false
I0506 03:41:22.035701   40539 start.go:129] hostinfo: {"hostname":"DESKTOP-H4IMVF3","uptime":11817,"bootTime":1746516265,"procs":46,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"4ee3b662-c448-44e2-b54c-02be3271f358"}
I0506 03:41:22.039638   40539 start.go:139] virtualization:  guest
I0506 03:41:22.057228   40539 out.go:177] üòÑ  minikube v1.35.0 on Ubuntu 24.04 (amd64)
I0506 03:41:22.089369   40539 notify.go:220] Checking for updates...
I0506 03:41:22.092377   40539 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0506 03:41:22.106277   40539 driver.go:394] Setting default libvirt URI to qemu:///system
I0506 03:41:22.269925   40539 docker.go:123] docker version: linux-28.1.1:Docker Engine - Community
I0506 03:41:22.270008   40539 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0506 03:41:23.035046   40539 info.go:266] docker info: {ID:c480f2f4-489a-4aae-91e0-a4d996a31f77 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:true NGoroutines:49 SystemTime:2025-05-06 03:41:23.013895642 -0700 PDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4052606976 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-H4IMVF3 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0506 03:41:23.035203   40539 docker.go:318] overlay module found
I0506 03:41:23.047076   40539 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0506 03:41:23.057627   40539 start.go:297] selected driver: docker
I0506 03:41:23.057641   40539 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aryan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0506 03:41:23.057743   40539 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0506 03:41:23.057902   40539 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0506 03:41:23.249412   40539 info.go:266] docker info: {ID:c480f2f4-489a-4aae-91e0-a4d996a31f77 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:31 OomKillDisable:true NGoroutines:49 SystemTime:2025-05-06 03:41:23.236525591 -0700 PDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4052606976 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-H4IMVF3 Labels:[] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1]] Warnings:<nil>}}
I0506 03:41:23.249912   40539 cni.go:84] Creating CNI manager for ""
I0506 03:41:23.251512   40539 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0506 03:41:23.251607   40539 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aryan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0506 03:41:23.263481   40539 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0506 03:41:23.274985   40539 cache.go:121] Beginning downloading kic base image for docker with docker
I0506 03:41:23.285570   40539 out.go:177] üöú  Pulling base image v0.0.46 ...
I0506 03:41:23.299125   40539 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0506 03:41:23.299989   40539 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0506 03:41:23.300059   40539 preload.go:146] Found local preload: /home/aryan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0506 03:41:23.300081   40539 cache.go:56] Caching tarball of preloaded images
I0506 03:41:23.301413   40539 preload.go:172] Found /home/aryan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0506 03:41:23.301447   40539 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0506 03:41:23.301681   40539 profile.go:143] Saving config to /home/aryan/.minikube/profiles/minikube/config.json ...
I0506 03:41:23.459547   40539 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0506 03:41:23.459562   40539 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0506 03:41:23.459589   40539 cache.go:227] Successfully downloaded all kic artifacts
I0506 03:41:23.459621   40539 start.go:360] acquireMachinesLock for minikube: {Name:mk4ecb953ca8351ef6f950be59ddd07e9777736c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0506 03:41:23.459923   40539 start.go:364] duration metric: took 277.886¬µs to acquireMachinesLock for "minikube"
I0506 03:41:23.459962   40539 start.go:96] Skipping create...Using existing machine configuration
I0506 03:41:23.459978   40539 fix.go:54] fixHost starting: 
I0506 03:41:23.460298   40539 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0506 03:41:23.570047   40539 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0506 03:41:23.570083   40539 fix.go:138] unexpected machine state, will restart: <nil>
I0506 03:41:23.589502   40539 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0506 03:41:23.600431   40539 cli_runner.go:164] Run: docker start minikube
I0506 03:41:24.437711   40539 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0506 03:41:24.592663   40539 kic.go:430] container "minikube" state is running.
I0506 03:41:24.598278   40539 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0506 03:41:24.698375   40539 profile.go:143] Saving config to /home/aryan/.minikube/profiles/minikube/config.json ...
I0506 03:41:24.698683   40539 machine.go:93] provisionDockerMachine start ...
I0506 03:41:24.698780   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:24.802702   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:24.803751   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:24.803762   40539 main.go:141] libmachine: About to run SSH command:
hostname
I0506 03:41:24.807427   40539 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:45212->127.0.0.1:32768: read: connection reset by peer
I0506 03:41:28.141482   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0506 03:41:28.141708   40539 ubuntu.go:169] provisioning hostname "minikube"
I0506 03:41:28.142257   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:28.255762   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:28.256020   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:28.256030   40539 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0506 03:41:28.579311   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0506 03:41:28.579913   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:28.738188   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:28.738569   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:28.738599   40539 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0506 03:41:28.975627   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0506 03:41:28.975714   40539 ubuntu.go:175] set auth options {CertDir:/home/aryan/.minikube CaCertPath:/home/aryan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aryan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aryan/.minikube/machines/server.pem ServerKeyPath:/home/aryan/.minikube/machines/server-key.pem ClientKeyPath:/home/aryan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aryan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aryan/.minikube}
I0506 03:41:28.976446   40539 ubuntu.go:177] setting up certificates
I0506 03:41:28.976484   40539 provision.go:84] configureAuth start
I0506 03:41:28.976689   40539 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0506 03:41:29.098305   40539 provision.go:143] copyHostCerts
I0506 03:41:29.098367   40539 exec_runner.go:144] found /home/aryan/.minikube/ca.pem, removing ...
I0506 03:41:29.099308   40539 exec_runner.go:203] rm: /home/aryan/.minikube/ca.pem
I0506 03:41:29.100053   40539 exec_runner.go:151] cp: /home/aryan/.minikube/certs/ca.pem --> /home/aryan/.minikube/ca.pem (1074 bytes)
I0506 03:41:29.100708   40539 exec_runner.go:144] found /home/aryan/.minikube/cert.pem, removing ...
I0506 03:41:29.100716   40539 exec_runner.go:203] rm: /home/aryan/.minikube/cert.pem
I0506 03:41:29.100785   40539 exec_runner.go:151] cp: /home/aryan/.minikube/certs/cert.pem --> /home/aryan/.minikube/cert.pem (1119 bytes)
I0506 03:41:29.101513   40539 exec_runner.go:144] found /home/aryan/.minikube/key.pem, removing ...
I0506 03:41:29.101522   40539 exec_runner.go:203] rm: /home/aryan/.minikube/key.pem
I0506 03:41:29.101565   40539 exec_runner.go:151] cp: /home/aryan/.minikube/certs/key.pem --> /home/aryan/.minikube/key.pem (1675 bytes)
I0506 03:41:29.102111   40539 provision.go:117] generating server cert: /home/aryan/.minikube/machines/server.pem ca-key=/home/aryan/.minikube/certs/ca.pem private-key=/home/aryan/.minikube/certs/ca-key.pem org=aryan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0506 03:41:29.270880   40539 provision.go:177] copyRemoteCerts
I0506 03:41:29.272432   40539 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0506 03:41:29.272526   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:29.373208   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:29.502802   40539 ssh_runner.go:362] scp /home/aryan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0506 03:41:29.584217   40539 ssh_runner.go:362] scp /home/aryan/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0506 03:41:29.632218   40539 ssh_runner.go:362] scp /home/aryan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0506 03:41:29.670597   40539 provision.go:87] duration metric: took 694.09622ms to configureAuth
I0506 03:41:29.670617   40539 ubuntu.go:193] setting minikube options for container-runtime
I0506 03:41:29.670829   40539 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0506 03:41:29.670883   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:29.742134   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:29.742298   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:29.742305   40539 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0506 03:41:29.921786   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0506 03:41:29.921803   40539 ubuntu.go:71] root file system type: overlay
I0506 03:41:29.921944   40539 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0506 03:41:29.922041   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:30.026673   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:30.026898   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:30.027156   40539 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0506 03:41:30.267476   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0506 03:41:30.267850   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:30.455623   40539 main.go:141] libmachine: Using SSH client type: native
I0506 03:41:30.455972   40539 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0506 03:41:30.455999   40539 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0506 03:41:30.816236   40539 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0506 03:41:30.816280   40539 machine.go:96] duration metric: took 6.117575416s to provisionDockerMachine
I0506 03:41:30.816313   40539 start.go:293] postStartSetup for "minikube" (driver="docker")
I0506 03:41:30.816366   40539 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0506 03:41:30.816862   40539 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0506 03:41:30.817081   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:31.014388   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:31.177927   40539 ssh_runner.go:195] Run: cat /etc/os-release
I0506 03:41:31.195886   40539 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0506 03:41:31.196005   40539 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0506 03:41:31.196047   40539 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0506 03:41:31.196070   40539 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0506 03:41:31.196101   40539 filesync.go:126] Scanning /home/aryan/.minikube/addons for local assets ...
I0506 03:41:31.197839   40539 filesync.go:126] Scanning /home/aryan/.minikube/files for local assets ...
I0506 03:41:31.199524   40539 start.go:296] duration metric: took 383.179356ms for postStartSetup
I0506 03:41:31.199925   40539 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0506 03:41:31.200055   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:31.376250   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:31.516891   40539 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0506 03:41:31.544156   40539 fix.go:56] duration metric: took 8.084158991s for fixHost
I0506 03:41:31.544192   40539 start.go:83] releasing machines lock for "minikube", held for 8.084250845s
I0506 03:41:31.544399   40539 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0506 03:41:31.816258   40539 ssh_runner.go:195] Run: cat /version.json
I0506 03:41:31.816330   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:31.816408   40539 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0506 03:41:31.816461   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:31.970325   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:31.978001   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:32.134374   40539 ssh_runner.go:195] Run: systemctl --version
I0506 03:41:32.935145   40539 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.118661429s)
I0506 03:41:32.935804   40539 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0506 03:41:32.957709   40539 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0506 03:41:33.011065   40539 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0506 03:41:33.011169   40539 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0506 03:41:33.025400   40539 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0506 03:41:33.025420   40539 start.go:495] detecting cgroup driver to use...
I0506 03:41:33.025459   40539 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0506 03:41:33.025628   40539 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0506 03:41:33.047662   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0506 03:41:33.092778   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0506 03:41:33.116357   40539 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0506 03:41:33.116477   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0506 03:41:33.137647   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0506 03:41:33.151763   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0506 03:41:33.201694   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0506 03:41:33.226180   40539 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0506 03:41:33.268574   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0506 03:41:33.298255   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0506 03:41:33.317953   40539 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0506 03:41:33.335635   40539 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0506 03:41:33.361721   40539 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0506 03:41:33.378272   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:33.634590   40539 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0506 03:41:33.846915   40539 start.go:495] detecting cgroup driver to use...
I0506 03:41:33.846956   40539 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0506 03:41:33.847010   40539 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0506 03:41:33.868409   40539 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0506 03:41:33.868479   40539 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0506 03:41:33.891423   40539 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0506 03:41:33.924846   40539 ssh_runner.go:195] Run: which cri-dockerd
I0506 03:41:33.967007   40539 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0506 03:41:34.010036   40539 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0506 03:41:34.042235   40539 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0506 03:41:34.479533   40539 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0506 03:41:34.969938   40539 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0506 03:41:34.970091   40539 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0506 03:41:34.996640   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:35.493482   40539 ssh_runner.go:195] Run: sudo systemctl restart docker
I0506 03:41:42.015871   40539 ssh_runner.go:235] Completed: sudo systemctl restart docker: (6.522358479s)
I0506 03:41:42.015949   40539 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0506 03:41:42.029942   40539 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0506 03:41:42.048730   40539 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0506 03:41:42.063517   40539 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0506 03:41:42.411370   40539 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0506 03:41:42.642089   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:42.985826   40539 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0506 03:41:43.003804   40539 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0506 03:41:43.019143   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:43.198682   40539 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0506 03:41:43.908850   40539 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0506 03:41:43.908917   40539 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0506 03:41:43.915315   40539 start.go:563] Will wait 60s for crictl version
I0506 03:41:43.915376   40539 ssh_runner.go:195] Run: which crictl
I0506 03:41:43.922104   40539 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0506 03:41:44.470656   40539 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0506 03:41:44.470761   40539 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0506 03:41:45.079964   40539 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0506 03:41:45.136736   40539 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0506 03:41:45.137261   40539 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0506 03:41:45.254149   40539 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0506 03:41:45.259355   40539 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0506 03:41:45.275699   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0506 03:41:45.399341   40539 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aryan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0506 03:41:45.399513   40539 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0506 03:41:45.399603   40539 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0506 03:41:45.426123   40539 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0506 03:41:45.426139   40539 docker.go:619] Images already preloaded, skipping extraction
I0506 03:41:45.426193   40539 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0506 03:41:45.457217   40539 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0506 03:41:45.457232   40539 cache_images.go:84] Images are preloaded, skipping loading
I0506 03:41:45.457241   40539 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0506 03:41:45.457430   40539 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0506 03:41:45.457555   40539 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0506 03:41:46.337961   40539 cni.go:84] Creating CNI manager for ""
I0506 03:41:46.338002   40539 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0506 03:41:46.338053   40539 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0506 03:41:46.338074   40539 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0506 03:41:46.338292   40539 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0506 03:41:46.338390   40539 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0506 03:41:46.353986   40539 binaries.go:44] Found k8s binaries, skipping transfer
I0506 03:41:46.354059   40539 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0506 03:41:46.366244   40539 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0506 03:41:46.389970   40539 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0506 03:41:46.414674   40539 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0506 03:41:46.486367   40539 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0506 03:41:46.504460   40539 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0506 03:41:46.529913   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:46.732170   40539 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0506 03:41:46.756395   40539 certs.go:68] Setting up /home/aryan/.minikube/profiles/minikube for IP: 192.168.49.2
I0506 03:41:46.756406   40539 certs.go:194] generating shared ca certs ...
I0506 03:41:46.756418   40539 certs.go:226] acquiring lock for ca certs: {Name:mk37f3a14f273a957a040ccc98f8eb76064b1071 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0506 03:41:46.759601   40539 certs.go:235] skipping valid "minikubeCA" ca cert: /home/aryan/.minikube/ca.key
I0506 03:41:46.760871   40539 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/aryan/.minikube/proxy-client-ca.key
I0506 03:41:46.760888   40539 certs.go:256] generating profile certs ...
I0506 03:41:46.761844   40539 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/aryan/.minikube/profiles/minikube/client.key
I0506 03:41:46.764130   40539 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/aryan/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0506 03:41:46.766096   40539 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/aryan/.minikube/profiles/minikube/proxy-client.key
I0506 03:41:46.769316   40539 certs.go:484] found cert: /home/aryan/.minikube/certs/ca-key.pem (1675 bytes)
I0506 03:41:46.769480   40539 certs.go:484] found cert: /home/aryan/.minikube/certs/ca.pem (1074 bytes)
I0506 03:41:46.769593   40539 certs.go:484] found cert: /home/aryan/.minikube/certs/cert.pem (1119 bytes)
I0506 03:41:46.769644   40539 certs.go:484] found cert: /home/aryan/.minikube/certs/key.pem (1675 bytes)
I0506 03:41:46.774602   40539 ssh_runner.go:362] scp /home/aryan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0506 03:41:46.811967   40539 ssh_runner.go:362] scp /home/aryan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0506 03:41:49.678905   40539 ssh_runner.go:362] scp /home/aryan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0506 03:41:49.721898   40539 ssh_runner.go:362] scp /home/aryan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0506 03:41:49.761690   40539 ssh_runner.go:362] scp /home/aryan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0506 03:41:49.808634   40539 ssh_runner.go:362] scp /home/aryan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0506 03:41:49.867301   40539 ssh_runner.go:362] scp /home/aryan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0506 03:41:49.935916   40539 ssh_runner.go:362] scp /home/aryan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0506 03:41:50.015363   40539 ssh_runner.go:362] scp /home/aryan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0506 03:41:50.053467   40539 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0506 03:41:50.085150   40539 ssh_runner.go:195] Run: openssl version
I0506 03:41:50.120314   40539 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0506 03:41:50.199625   40539 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0506 03:41:50.212871   40539 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  6 08:35 /usr/share/ca-certificates/minikubeCA.pem
I0506 03:41:50.213139   40539 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0506 03:41:50.227987   40539 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0506 03:41:50.247689   40539 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0506 03:41:50.253579   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0506 03:41:50.283238   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0506 03:41:50.315658   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0506 03:41:50.340272   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0506 03:41:50.366369   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0506 03:41:50.391721   40539 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0506 03:41:50.408705   40539 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aryan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0506 03:41:50.408944   40539 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0506 03:41:50.469378   40539 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0506 03:41:50.486840   40539 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0506 03:41:50.486854   40539 kubeadm.go:593] restartPrimaryControlPlane start ...
I0506 03:41:50.486925   40539 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0506 03:41:50.541425   40539 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0506 03:41:50.541642   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0506 03:41:50.685407   40539 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/aryan/.kube/config
I0506 03:41:50.685542   40539 kubeconfig.go:62] /home/aryan/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0506 03:41:50.685864   40539 lock.go:35] WriteFile acquiring /home/aryan/.kube/config: {Name:mk58d84c28163d1600481f26b6ef5a8b8c847c0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0506 03:41:50.745006   40539 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0506 03:41:50.772851   40539 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0506 03:41:50.772892   40539 kubeadm.go:597] duration metric: took 286.025145ms to restartPrimaryControlPlane
I0506 03:41:50.772909   40539 kubeadm.go:394] duration metric: took 364.21077ms to StartCluster
I0506 03:41:50.772938   40539 settings.go:142] acquiring lock: {Name:mk295fe464b94058302a0097c044aad6394f8d96 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0506 03:41:50.773163   40539 settings.go:150] Updating kubeconfig:  /home/aryan/.kube/config
I0506 03:41:50.774548   40539 lock.go:35] WriteFile acquiring /home/aryan/.kube/config: {Name:mk58d84c28163d1600481f26b6ef5a8b8c847c0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0506 03:41:50.775354   40539 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0506 03:41:50.775826   40539 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0506 03:41:50.775647   40539 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0506 03:41:50.775940   40539 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0506 03:41:50.775965   40539 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0506 03:41:50.775993   40539 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0506 03:41:50.776013   40539 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0506 03:41:50.776029   40539 addons.go:247] addon storage-provisioner should already be in state true
I0506 03:41:50.776079   40539 host.go:66] Checking if "minikube" exists ...
I0506 03:41:50.776690   40539 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0506 03:41:50.777812   40539 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0506 03:41:50.799861   40539 out.go:177] üîé  Verifying Kubernetes components...
I0506 03:41:50.814896   40539 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0506 03:41:50.877799   40539 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0506 03:41:50.877817   40539 addons.go:247] addon default-storageclass should already be in state true
I0506 03:41:50.877851   40539 host.go:66] Checking if "minikube" exists ...
I0506 03:41:50.878337   40539 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0506 03:41:50.886461   40539 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0506 03:41:50.905584   40539 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:50.905596   40539 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0506 03:41:50.905683   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:51.014809   40539 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:51.014823   40539 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0506 03:41:51.014888   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0506 03:41:51.021128   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:51.056038   40539 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0506 03:41:51.087468   40539 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0506 03:41:51.154684   40539 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/aryan/.minikube/machines/minikube/id_rsa Username:docker}
I0506 03:41:51.190434   40539 api_server.go:52] waiting for apiserver process to appear ...
I0506 03:41:51.190521   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:51.214313   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:51.574638   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:51.691771   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:52.882694   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.308022415s)
I0506 03:41:52.882704   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.668375576s)
W0506 03:41:52.882720   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0506 03:41:52.882720   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:52.882799   40539 retry.go:31] will retry after 184.38835ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:52.882804   40539 retry.go:31] will retry after 231.774931ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:52.882846   40539 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.191056195s)
I0506 03:41:52.882929   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:53.067675   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:53.115546   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:53.192549   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:54.082334   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.014587852s)
W0506 03:41:54.082412   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:54.082457   40539 retry.go:31] will retry after 255.681139ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:54.339284   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:54.711190   40539 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.518576645s)
I0506 03:41:54.711411   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:54.712664   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.597050562s)
W0506 03:41:54.712724   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:54.712760   40539 retry.go:31] will retry after 238.112904ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:54.952203   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:55.773341   40539 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.06186304s)
I0506 03:41:55.773566   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:55.774569   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.435218815s)
W0506 03:41:55.774640   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:55.774684   40539 retry.go:31] will retry after 670.341869ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0506 03:41:55.817890   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:55.817968   40539 retry.go:31] will retry after 744.574372ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:56.191412   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:56.446387   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:56.563791   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:56.691620   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:58.413050   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.966489355s)
W0506 03:41:58.413143   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:58.413198   40539 retry.go:31] will retry after 705.603772ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:58.413589   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.849736528s)
W0506 03:41:58.413712   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:58.413789   40539 retry.go:31] will retry after 432.966597ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:58.415198   40539 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.722111507s)
I0506 03:41:58.415464   40539 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0506 03:41:58.677141   40539 api_server.go:72] duration metric: took 7.901710524s to wait for apiserver process to appear ...
I0506 03:41:58.677189   40539 api_server.go:88] waiting for apiserver healthz status ...
I0506 03:41:58.677326   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:41:58.687942   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0506 03:41:58.847906   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:41:59.119868   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:41:59.178317   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:41:59.186311   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
W0506 03:41:59.597442   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:59.597503   40539 retry.go:31] will retry after 1.609529176s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:59.677999   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:41:59.684185   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": read tcp 127.0.0.1:45314->127.0.0.1:32771: read: connection reset by peer
W0506 03:41:59.738843   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:41:59.738927   40539 retry.go:31] will retry after 1.195653518s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:42:00.177800   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:00.190782   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": read tcp 127.0.0.1:45330->127.0.0.1:32771: read: connection reset by peer
I0506 03:42:00.678202   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:00.679812   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0506 03:42:00.936128   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0506 03:42:01.071058   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:42:01.071083   40539 retry.go:31] will retry after 1.553365279s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:42:01.178204   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:01.179447   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0506 03:42:01.207464   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0506 03:42:01.596992   40539 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:42:01.597059   40539 retry.go:31] will retry after 2.156231455s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0506 03:42:01.678402   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:01.680038   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0506 03:42:02.178211   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:02.180240   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": read tcp 127.0.0.1:45372->127.0.0.1:32771: read: connection reset by peer
I0506 03:42:02.624730   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0506 03:42:02.677830   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:02.680091   40539 api_server.go:269] stopped: https://127.0.0.1:32771/healthz: Get "https://127.0.0.1:32771/healthz": EOF
I0506 03:42:03.178282   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:03.754294   40539 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0506 03:42:07.904851   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0506 03:42:07.904877   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0506 03:42:07.904903   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:07.993436   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0506 03:42:07.993457   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0506 03:42:08.177792   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:08.290746   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:08.290767   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:08.678364   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:08.700613   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:08.700693   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:09.178414   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:09.204262   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:09.204316   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:09.678317   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:09.707077   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:09.707181   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:10.182261   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:10.208284   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:10.208531   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:10.678081   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:10.702996   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:10.703094   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:11.177511   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:11.285632   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:11.285693   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:11.680261   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:11.811043   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0506 03:42:11.811098   40539 api_server.go:103] status: https://127.0.0.1:32771/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0506 03:42:12.177415   40539 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I0506 03:42:12.198640   40539 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I0506 03:42:12.275102   40539 api_server.go:141] control plane version: v1.32.0
I0506 03:42:12.275285   40539 api_server.go:131] duration metric: took 13.598016487s to wait for apiserver health ...
I0506 03:42:12.275392   40539 system_pods.go:43] waiting for kube-system pods to appear ...
I0506 03:42:12.443766   40539 system_pods.go:59] 7 kube-system pods found
I0506 03:42:12.443823   40539 system_pods.go:61] "coredns-668d6bf9bc-qmlqr" [bc9baf53-163e-42f4-81a1-6534545b94bb] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0506 03:42:12.443844   40539 system_pods.go:61] "etcd-minikube" [d4320bb3-5824-482b-a107-fe8a1e31401d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0506 03:42:12.443861   40539 system_pods.go:61] "kube-apiserver-minikube" [fd81d1ba-ff3c-4984-91f8-014dad90238d] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0506 03:42:12.443878   40539 system_pods.go:61] "kube-controller-manager-minikube" [4d631225-351b-425e-99ce-a6c998c4747a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0506 03:42:12.443893   40539 system_pods.go:61] "kube-proxy-gp8k9" [86118735-17a1-47ed-9cdf-de104a79fc3a] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0506 03:42:12.443902   40539 system_pods.go:61] "kube-scheduler-minikube" [f0217e33-30c5-4bf3-b00e-d103c6887e0f] Running
I0506 03:42:12.443915   40539 system_pods.go:61] "storage-provisioner" [44cd927a-e72a-4aa6-be8b-d89a94e2486a] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0506 03:42:12.443931   40539 system_pods.go:74] duration metric: took 168.521026ms to wait for pod list to return data ...
I0506 03:42:12.443961   40539 kubeadm.go:582] duration metric: took 21.668548439s to wait for: map[apiserver:true system_pods:true]
I0506 03:42:12.443996   40539 node_conditions.go:102] verifying NodePressure condition ...
I0506 03:42:12.521038   40539 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0506 03:42:12.521099   40539 node_conditions.go:123] node cpu capacity is 4
I0506 03:42:12.521134   40539 node_conditions.go:105] duration metric: took 77.125133ms to run NodePressure ...
I0506 03:42:12.521179   40539 start.go:241] waiting for startup goroutines ...
I0506 03:42:13.384211   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.759401414s)
I0506 03:42:13.385685   40539 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (9.631319076s)
I0506 03:42:13.539346   40539 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0506 03:42:13.551830   40539 addons.go:514] duration metric: took 22.776224535s for enable addons: enabled=[storage-provisioner default-storageclass]
I0506 03:42:13.552433   40539 start.go:246] waiting for cluster config update ...
I0506 03:42:13.552571   40539 start.go:255] writing updated cluster config ...
I0506 03:42:13.553675   40539 ssh_runner.go:195] Run: rm -f paused
I0506 03:42:31.299914   40539 start.go:600] kubectl: 1.32.4, cluster: 1.32.0 (minor skew: 0)
I0506 03:42:31.309778   40539 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Start docker client with request timeout 0s"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Loaded network plugin cni"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Docker cri networking managed by network plugin cni"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Setting cgroupDriver cgroupfs"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 06 10:41:43 minikube cri-dockerd[1377]: time="2025-05-06T10:41:43Z" level=info msg="Start cri-dockerd grpc backend"
May 06 10:41:43 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 06 10:41:51 minikube cri-dockerd[1377]: time="2025-05-06T10:41:51Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-qmlqr_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1a56b6c84db8ef7582029808b650e960119ba69ef6d761ee42334d293e9ebcec\""
May 06 10:41:54 minikube cri-dockerd[1377]: time="2025-05-06T10:41:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/957472c96ca045817295acec48f8888fa1434671dc50244a825ca07aa7aae3fc/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:41:55 minikube cri-dockerd[1377]: time="2025-05-06T10:41:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e301cedf078062faf4a3e9253e341ad2604abd88db80be8dbef2ea7071149d3/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:41:55 minikube cri-dockerd[1377]: time="2025-05-06T10:41:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1c5bb98ddd7b247771729201fe9b68f9e6b46622a81221a56f7fcadbced593a0/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:41:55 minikube cri-dockerd[1377]: time="2025-05-06T10:41:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/41d7ef3e6dbada0575f9e85d9cf959d1a0db6b67a341a5c4258b6c12e8949f6b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:42:08 minikube cri-dockerd[1377]: time="2025-05-06T10:42:08Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 06 10:42:13 minikube cri-dockerd[1377]: time="2025-05-06T10:42:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b8a297e9a03982c4c26a1dc7eed35d3a5f4e7e3276688d17528ccc77d36d78fe/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:42:14 minikube cri-dockerd[1377]: time="2025-05-06T10:42:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/677eff4c9619e0f4e1eefc9d5a949a8b4f8535eb1b4d7bb3e3e395dafac53549/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:42:14 minikube cri-dockerd[1377]: time="2025-05-06T10:42:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2221801dd3e0bb34192fae66a814261efbd1ed2cea68638d10f9263afa7675e0/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 06 10:42:51 minikube dockerd[1090]: time="2025-05-06T10:42:51.295476662Z" level=info msg="ignoring event" container=7b5923eda65b5ea0df78e5dda3024eb7e188c0a3155b26c9093abb02d74c082b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 06 10:44:37 minikube cri-dockerd[1377]: time="2025-05-06T10:44:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7510892242ebfe888f0e1bf7ab1ea51d5709c60a4e41f71bb06e1767ef225052/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 06 10:44:40 minikube dockerd[1090]: time="2025-05-06T10:44:40.686584537Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:44:40 minikube dockerd[1090]: time="2025-05-06T10:44:40.686808387Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:44:45 minikube dockerd[1090]: time="2025-05-06T10:44:45.176568920Z" level=info msg="ignoring event" container=7510892242ebfe888f0e1bf7ab1ea51d5709c60a4e41f71bb06e1767ef225052 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 06 10:44:46 minikube cri-dockerd[1377]: time="2025-05-06T10:44:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/01ad8d3e1f8e40d4c25ee8405d498eaa88229cdbc6b69ffcba4fb8ed2977d389/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 06 10:45:04 minikube dockerd[1090]: time="2025-05-06T10:45:04.377819168Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:45:04 minikube dockerd[1090]: time="2025-05-06T10:45:04.377921560Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:45:35 minikube dockerd[1090]: time="2025-05-06T10:45:35.108705738Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:45:35 minikube dockerd[1090]: time="2025-05-06T10:45:35.109181946Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:46:34 minikube dockerd[1090]: time="2025-05-06T10:46:34.252631810Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:46:34 minikube dockerd[1090]: time="2025-05-06T10:46:34.252980156Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:48:08 minikube dockerd[1090]: time="2025-05-06T10:48:08.246068655Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:48:08 minikube dockerd[1090]: time="2025-05-06T10:48:08.246235515Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:51:24 minikube dockerd[1090]: time="2025-05-06T10:51:24.811931097Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:51:24 minikube dockerd[1090]: time="2025-05-06T10:51:24.812198127Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:53:42 minikube cri-dockerd[1377]: time="2025-05-06T10:53:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3aef4bda7b16d1d8edd9c9f2f65c7cca71ffd84a96a13487c73333818adb8479/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 06 10:53:46 minikube dockerd[1090]: time="2025-05-06T10:53:46.369673933Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:53:46 minikube dockerd[1090]: time="2025-05-06T10:53:46.369782473Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:54:02 minikube dockerd[1090]: time="2025-05-06T10:54:02.155708763Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:54:02 minikube dockerd[1090]: time="2025-05-06T10:54:02.156025203Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:54:33 minikube dockerd[1090]: time="2025-05-06T10:54:33.734707385Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:54:33 minikube dockerd[1090]: time="2025-05-06T10:54:33.734874155Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:55:21 minikube dockerd[1090]: time="2025-05-06T10:55:21.348855854Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:55:21 minikube dockerd[1090]: time="2025-05-06T10:55:21.349112804Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:57:00 minikube dockerd[1090]: time="2025-05-06T10:57:00.124593815Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:57:00 minikube dockerd[1090]: time="2025-05-06T10:57:00.124852745Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:57:03 minikube dockerd[1090]: time="2025-05-06T10:57:03.018020195Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 10:57:03 minikube dockerd[1090]: time="2025-05-06T10:57:03.018128555Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 10:59:48 minikube dockerd[1090]: time="2025-05-06T10:59:48.407162703Z" level=info msg="ignoring event" container=01ad8d3e1f8e40d4c25ee8405d498eaa88229cdbc6b69ffcba4fb8ed2977d389 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 06 10:59:49 minikube cri-dockerd[1377]: time="2025-05-06T10:59:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8dd67aa73f6f997fb8907e18c3dacbeb315e74bd94b9dbc12b4a5d87aba37bd4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 06 10:59:57 minikube dockerd[1090]: time="2025-05-06T10:59:57.358666293Z" level=warning msg="Error getting v2 registry: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 10:59:57 minikube dockerd[1090]: time="2025-05-06T10:59:57.360663933Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 10:59:57 minikube dockerd[1090]: time="2025-05-06T10:59:57.374690253Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:00:18 minikube dockerd[1090]: time="2025-05-06T11:00:18.192547847Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 06 11:00:18 minikube dockerd[1090]: time="2025-05-06T11:00:18.192637667Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 06 11:00:25 minikube dockerd[1090]: time="2025-05-06T11:00:25.647067847Z" level=warning msg="Error getting v2 registry: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:00:25 minikube dockerd[1090]: time="2025-05-06T11:00:25.647167297Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:00:25 minikube dockerd[1090]: time="2025-05-06T11:00:25.660105337Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:01:03 minikube dockerd[1090]: time="2025-05-06T11:01:03.846390649Z" level=warning msg="Error getting v2 registry: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:01:03 minikube dockerd[1090]: time="2025-05-06T11:01:03.846640309Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"
May 06 11:01:03 minikube dockerd[1090]: time="2025-05-06T11:01:03.864890689Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
dbdaf4c278d3a       6e38f40d628db       18 minutes ago      Running             storage-provisioner       2                   677eff4c9619e       storage-provisioner
1c223a24b57f9       c69fa2e9cbf5f       19 minutes ago      Running             coredns                   1                   2221801dd3e0b       coredns-668d6bf9bc-qmlqr
7b5923eda65b5       6e38f40d628db       19 minutes ago      Exited              storage-provisioner       1                   677eff4c9619e       storage-provisioner
eb0817294c163       040f9f8aac8cd       19 minutes ago      Running             kube-proxy                1                   b8a297e9a0398       kube-proxy-gp8k9
f08374ee2e019       a9e7e6b294baf       19 minutes ago      Running             etcd                      1                   41d7ef3e6dbad       etcd-minikube
48649d962f2c5       c2e17b8d0f4a3       19 minutes ago      Running             kube-apiserver            1                   1c5bb98ddd7b2       kube-apiserver-minikube
c55e16abc80fb       a389e107f4ff1       19 minutes ago      Running             kube-scheduler            1                   8e301cedf0780       kube-scheduler-minikube
dce153cffada0       8cab3d2a8bd0f       19 minutes ago      Running             kube-controller-manager   1                   957472c96ca04       kube-controller-manager-minikube
a03675fb019df       c69fa2e9cbf5f       About an hour ago   Exited              coredns                   0                   1a56b6c84db8e       coredns-668d6bf9bc-qmlqr
b593fb53167c7       040f9f8aac8cd       About an hour ago   Exited              kube-proxy                0                   23923fe616603       kube-proxy-gp8k9
e75ff50645b18       a389e107f4ff1       About an hour ago   Exited              kube-scheduler            0                   86631f1201d38       kube-scheduler-minikube
cac9fafba2fbf       8cab3d2a8bd0f       About an hour ago   Exited              kube-controller-manager   0                   49f1396f425fd       kube-controller-manager-minikube
35672d8a15aeb       c2e17b8d0f4a3       About an hour ago   Exited              kube-apiserver            0                   94d84ff5d4cf7       kube-apiserver-minikube
a1fae9ea5c597       a9e7e6b294baf       About an hour ago   Exited              etcd                      0                   c2f50656a8fc9       etcd-minikube


==> coredns [1c223a24b57f] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:53186 - 61125 "HINFO IN 7213781169879739191.7262445915178790910. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.060911209s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[650160519]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 10:42:25.175) (total time: 30093ms):
Trace[650160519]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30026ms (10:42:55.202)
Trace[650160519]: [30.093506018s] [30.093506018s] END
[INFO] plugin/kubernetes: Trace[609274924]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 10:42:25.187) (total time: 30082ms):
Trace[609274924]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30014ms (10:42:55.202)
Trace[609274924]: [30.082160892s] [30.082160892s] END
[INFO] plugin/kubernetes: Trace[1326181505]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 10:42:25.190) (total time: 30078ms):
Trace[1326181505]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30012ms (10:42:55.202)
Trace[1326181505]: [30.0783691s] [30.0783691s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [a03675fb019d] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2036509955]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 09:37:16.845) (total time: 30002ms):
Trace[2036509955]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (09:37:49.777)
Trace[2036509955]: [30.002555882s] [30.002555882s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1878198826]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 09:37:16.847) (total time: 30000ms):
Trace[1878198826]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:37:49.777)
Trace[1878198826]: [30.000804223s] [30.000804223s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[887970808]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-May-2025 09:37:16.848) (total time: 30000ms):
Trace[887970808]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (09:37:49.778)
Trace[887970808]: [30.000875265s] [30.000875265s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
[INFO] Reloading complete
[INFO] 127.0.0.1:39462 - 19605 "HINFO IN 1234013169878939482.8944477484206808321. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.028543818s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_06T02_36_56_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 06 May 2025 09:36:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 06 May 2025 11:01:11 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 06 May 2025 10:59:58 +0000   Tue, 06 May 2025 09:36:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 06 May 2025 10:59:58 +0000   Tue, 06 May 2025 09:36:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 06 May 2025 10:59:58 +0000   Tue, 06 May 2025 09:36:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 06 May 2025 10:59:58 +0000   Tue, 06 May 2025 09:36:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3957624Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3957624Ki
  pods:               110
System Info:
  Machine ID:                 821d02202ae142f7bd4a3ba161299b49
  System UUID:                821d02202ae142f7bd4a3ba161299b49
  Boot ID:                    c542bd78-a787-4253-9b82-f0f709e7e793
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     website-deployment-7666cd884b-hpnc7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m37s
  default                     website-deployment-77946c4894-sjttb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         89s
  kube-system                 coredns-668d6bf9bc-qmlqr               100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     84m
  kube-system                 etcd-minikube                          100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         84m
  kube-system                 kube-apiserver-minikube                250m (6%)     0 (0%)      0 (0%)           0 (0%)         84m
  kube-system                 kube-controller-manager-minikube       200m (5%)     0 (0%)      0 (0%)           0 (0%)         84m
  kube-system                 kube-proxy-gp8k9                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m
  kube-system                 kube-scheduler-minikube                100m (2%)     0 (0%)      0 (0%)           0 (0%)         84m
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           83m                kube-proxy       
  Normal   Starting                           18m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  85m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           85m                kubelet          Starting kubelet.
  Warning  CgroupV1                           85m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            85m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            85m (x8 over 85m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              85m (x8 over 85m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               85m (x7 over 85m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  84m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           84m                kubelet          Starting kubelet.
  Warning  CgroupV1                           84m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            84m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            84m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              84m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               84m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     84m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  19m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           19m                kubelet          Starting kubelet.
  Warning  CgroupV1                           19m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               19m (x7 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            19m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     19m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May 6 07:26] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.000000] PCI: Fatal: No config space access function found
[  +0.030267] PCI: System does not support PCI
[  +0.026719] kvm: no hardware support
[  +0.000004] kvm: no hardware support
[  +1.711662] FS-Cache: Duplicate cookie detected
[  +0.001210] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.001479] FS-Cache: O-cookie d=000000004dae0483{9P.session} n=00000000f93d802b
[  +0.001642] FS-Cache: O-key=[10] '34323934393337343732'
[  +0.005748] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.005867] FS-Cache: N-cookie d=000000004dae0483{9P.session} n=000000001bd00440
[  +0.001745] FS-Cache: N-key=[10] '34323934393337343732'
[  +2.837413] Failed to connect to bus: No such file or directory
[  +0.257491] Failed to connect to bus: No such file or directory
[  +0.256345] Failed to connect to bus: No such file or directory
[  +0.143511] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.027384] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001108] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000889] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001244] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.084718] Failed to connect to bus: No such file or directory
[  +0.413087] systemd-journald[60]: File /var/log/journal/4ee3b662c44844e2b54c02be3271f358/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.479049] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +2.975374] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[  +2.264810] systemd-journald[60]: File /var/log/journal/4ee3b662c44844e2b54c02be3271f358/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[May 6 07:59] overlayfs: missing 'lowerdir'
[May 6 08:44] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 6 09:04] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 6 09:10] tmpfs: Unknown parameter 'noswap'
[ +20.390077] tmpfs: Unknown parameter 'noswap'
[May 6 09:47] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 6 10:11] tmpfs: Unknown parameter 'noswap'
[ +40.684591] tmpfs: Unknown parameter 'noswap'
[May 6 10:15] hrtimer: interrupt took 1381234 ns
[May 6 10:21] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 6 10:41] WSL (123) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 6 10:59] tmpfs: Unknown parameter 'noswap'


==> etcd [a1fae9ea5c59] <==
{"level":"info","ts":"2025-05-06T09:37:16.029634Z","caller":"traceutil/trace.go:171","msg":"trace[1742549284] transaction","detail":"{read_only:false; response_revision:422; number_of_response:1; }","duration":"167.837875ms","start":"2025-05-06T09:37:15.861740Z","end":"2025-05-06T09:37:16.029578Z","steps":["trace[1742549284] 'process raft request'  (duration: 64.511539ms)","trace[1742549284] 'compare'  (duration: 102.32726ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:37:18.330884Z","caller":"traceutil/trace.go:171","msg":"trace[533514809] transaction","detail":"{read_only:false; response_revision:434; number_of_response:1; }","duration":"113.085814ms","start":"2025-05-06T09:37:18.217755Z","end":"2025-05-06T09:37:18.330841Z","steps":["trace[533514809] 'process raft request'  (duration: 112.923381ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:37:18.331580Z","caller":"traceutil/trace.go:171","msg":"trace[964451004] transaction","detail":"{read_only:false; response_revision:433; number_of_response:1; }","duration":"120.349941ms","start":"2025-05-06T09:37:18.211190Z","end":"2025-05-06T09:37:18.331540Z","steps":["trace[964451004] 'process raft request'  (duration: 98.7397ms)","trace[964451004] 'compare'  (duration: 20.466058ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:37:18.527396Z","caller":"traceutil/trace.go:171","msg":"trace[841961718] transaction","detail":"{read_only:false; response_revision:436; number_of_response:1; }","duration":"111.597422ms","start":"2025-05-06T09:37:18.415753Z","end":"2025-05-06T09:37:18.527350Z","steps":["trace[841961718] 'process raft request'  (duration: 111.459005ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:37:18.528857Z","caller":"traceutil/trace.go:171","msg":"trace[118080209] transaction","detail":"{read_only:false; response_revision:435; number_of_response:1; }","duration":"174.389201ms","start":"2025-05-06T09:37:18.354422Z","end":"2025-05-06T09:37:18.528812Z","steps":["trace[118080209] 'process raft request'  (duration: 78.145585ms)","trace[118080209] 'compare'  (duration: 94.312016ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:37:21.640365Z","caller":"traceutil/trace.go:171","msg":"trace[1755487644] transaction","detail":"{read_only:false; response_revision:447; number_of_response:1; }","duration":"300.71737ms","start":"2025-05-06T09:37:21.339601Z","end":"2025-05-06T09:37:21.640318Z","steps":["trace[1755487644] 'process raft request'  (duration: 300.24492ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:37:21.640721Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T09:37:21.339522Z","time spent":"301.020971ms","remote":"127.0.0.1:40818","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:442 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-05-06T09:37:22.163437Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"220.316234ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:37:22.163629Z","caller":"traceutil/trace.go:171","msg":"trace[1768091651] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:447; }","duration":"220.519459ms","start":"2025-05-06T09:37:21.943090Z","end":"2025-05-06T09:37:22.163609Z","steps":["trace[1768091651] 'range keys from in-memory index tree'  (duration: 220.299368ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:37:49.426913Z","caller":"traceutil/trace.go:171","msg":"trace[295298997] transaction","detail":"{read_only:false; response_revision:472; number_of_response:1; }","duration":"156.537937ms","start":"2025-05-06T09:37:49.270355Z","end":"2025-05-06T09:37:49.426893Z","steps":["trace[295298997] 'process raft request'  (duration: 156.375321ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:39:52.338434Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"118.538424ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037062710760909 > lease_revoke:<id:70cc96a4f4159587>","response":"size:29"}
{"level":"info","ts":"2025-05-06T09:40:06.993408Z","caller":"traceutil/trace.go:171","msg":"trace[1568203687] linearizableReadLoop","detail":"{readStateIndex:620; appliedIndex:619; }","duration":"299.78182ms","start":"2025-05-06T09:40:06.693579Z","end":"2025-05-06T09:40:06.993361Z","steps":["trace[1568203687] 'read index received'  (duration: 299.345303ms)","trace[1568203687] 'applied index is now lower than readState.Index'  (duration: 434.867¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T09:40:06.993658Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.041145ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:40:06.993736Z","caller":"traceutil/trace.go:171","msg":"trace[1246333045] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:575; }","duration":"300.139687ms","start":"2025-05-06T09:40:06.693573Z","end":"2025-05-06T09:40:06.993713Z","steps":["trace[1246333045] 'agreement among raft nodes before linearized reading'  (duration: 299.998612ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:40:06.993809Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T09:40:06.693538Z","time spent":"300.247762ms","remote":"127.0.0.1:40670","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-06T09:40:06.993982Z","caller":"traceutil/trace.go:171","msg":"trace[1346448561] transaction","detail":"{read_only:false; response_revision:575; number_of_response:1; }","duration":"338.32345ms","start":"2025-05-06T09:40:06.655624Z","end":"2025-05-06T09:40:06.993947Z","steps":["trace[1346448561] 'process raft request'  (duration: 337.437216ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:40:06.994205Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T09:40:06.655592Z","time spent":"338.49835ms","remote":"127.0.0.1:40892","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:567 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-05-06T09:40:18.087160Z","caller":"traceutil/trace.go:171","msg":"trace[1789145610] transaction","detail":"{read_only:false; response_revision:585; number_of_response:1; }","duration":"101.08866ms","start":"2025-05-06T09:40:17.986010Z","end":"2025-05-06T09:40:18.087099Z","steps":["trace[1789145610] 'process raft request'  (duration: 47.14015ms)","trace[1789145610] 'compare'  (duration: 53.588168ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:40:28.219040Z","caller":"traceutil/trace.go:171","msg":"trace[1050153929] transaction","detail":"{read_only:false; response_revision:593; number_of_response:1; }","duration":"103.772661ms","start":"2025-05-06T09:40:28.115218Z","end":"2025-05-06T09:40:28.218991Z","steps":["trace[1050153929] 'process raft request'  (duration: 102.83271ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:40:35.605865Z","caller":"traceutil/trace.go:171","msg":"trace[442694578] linearizableReadLoop","detail":"{readStateIndex:647; appliedIndex:646; }","duration":"156.701837ms","start":"2025-05-06T09:40:35.449113Z","end":"2025-05-06T09:40:35.605815Z","steps":["trace[442694578] 'read index received'  (duration: 156.227829ms)","trace[442694578] 'applied index is now lower than readState.Index'  (duration: 471.533¬µs)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:40:35.606159Z","caller":"traceutil/trace.go:171","msg":"trace[509941101] transaction","detail":"{read_only:false; response_revision:597; number_of_response:1; }","duration":"280.294305ms","start":"2025-05-06T09:40:35.325836Z","end":"2025-05-06T09:40:35.606130Z","steps":["trace[509941101] 'process raft request'  (duration: 279.457571ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:40:35.607146Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.003505ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-06T09:40:35.607315Z","caller":"traceutil/trace.go:171","msg":"trace[1744265559] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:597; }","duration":"158.249996ms","start":"2025-05-06T09:40:35.449028Z","end":"2025-05-06T09:40:35.607278Z","steps":["trace[1744265559] 'agreement among raft nodes before linearized reading'  (duration: 157.912113ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:40:35.849687Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"142.548407ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/resourcequotas/\" range_end:\"/registry/resourcequotas0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:40:35.849854Z","caller":"traceutil/trace.go:171","msg":"trace[78886552] range","detail":"{range_begin:/registry/resourcequotas/; range_end:/registry/resourcequotas0; response_count:0; response_revision:597; }","duration":"142.87474ms","start":"2025-05-06T09:40:35.706946Z","end":"2025-05-06T09:40:35.849821Z","steps":["trace[78886552] 'count revisions from in-memory index tree'  (duration: 142.33519ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:40:41.999394Z","caller":"traceutil/trace.go:171","msg":"trace[916291603] transaction","detail":"{read_only:false; response_revision:602; number_of_response:1; }","duration":"177.686911ms","start":"2025-05-06T09:40:41.821668Z","end":"2025-05-06T09:40:41.999355Z","steps":["trace[916291603] 'process raft request'  (duration: 177.155703ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:40:51.906971Z","caller":"traceutil/trace.go:171","msg":"trace[404285471] transaction","detail":"{read_only:false; response_revision:609; number_of_response:1; }","duration":"240.018158ms","start":"2025-05-06T09:40:51.666900Z","end":"2025-05-06T09:40:51.906918Z","steps":["trace[404285471] 'process raft request'  (duration: 239.188116ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:00.646721Z","caller":"traceutil/trace.go:171","msg":"trace[2033275983] linearizableReadLoop","detail":"{readStateIndex:670; appliedIndex:669; }","duration":"100.688626ms","start":"2025-05-06T09:41:00.545984Z","end":"2025-05-06T09:41:00.646673Z","steps":["trace[2033275983] 'read index received'  (duration: 100.274293ms)","trace[2033275983] 'applied index is now lower than readState.Index'  (duration: 412.133¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T09:41:00.647042Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.026051ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:41:00.647144Z","caller":"traceutil/trace.go:171","msg":"trace[382131298] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:615; }","duration":"101.153468ms","start":"2025-05-06T09:41:00.545962Z","end":"2025-05-06T09:41:00.647116Z","steps":["trace[382131298] 'agreement among raft nodes before linearized reading'  (duration: 100.921368ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:00.647338Z","caller":"traceutil/trace.go:171","msg":"trace[1705230602] transaction","detail":"{read_only:false; response_revision:615; number_of_response:1; }","duration":"112.825847ms","start":"2025-05-06T09:41:00.534478Z","end":"2025-05-06T09:41:00.647304Z","steps":["trace[1705230602] 'process raft request'  (duration: 111.797439ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:01.859979Z","caller":"traceutil/trace.go:171","msg":"trace[852520178] linearizableReadLoop","detail":"{readStateIndex:671; appliedIndex:670; }","duration":"320.904577ms","start":"2025-05-06T09:41:01.538958Z","end":"2025-05-06T09:41:01.859863Z","steps":["trace[852520178] 'read index received'  (duration: 320.682468ms)","trace[852520178] 'applied index is now lower than readState.Index'  (duration: 220.825¬µs)"],"step_count":2}
{"level":"info","ts":"2025-05-06T09:41:01.860182Z","caller":"traceutil/trace.go:171","msg":"trace[350046342] transaction","detail":"{read_only:false; response_revision:616; number_of_response:1; }","duration":"655.928851ms","start":"2025-05-06T09:41:01.204235Z","end":"2025-05-06T09:41:01.860163Z","steps":["trace[350046342] 'process raft request'  (duration: 655.474276ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:41:01.860255Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"321.274819ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-05-06T09:41:01.860327Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T09:41:01.204214Z","time spent":"656.019875ms","remote":"127.0.0.1:40892","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:608 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-05-06T09:41:01.860346Z","caller":"traceutil/trace.go:171","msg":"trace[1435857835] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:616; }","duration":"321.372168ms","start":"2025-05-06T09:41:01.538950Z","end":"2025-05-06T09:41:01.860322Z","steps":["trace[1435857835] 'agreement among raft nodes before linearized reading'  (duration: 321.25016ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:41:01.860252Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"225.846119ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:41:01.860475Z","caller":"traceutil/trace.go:171","msg":"trace[1365702652] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:616; }","duration":"226.113328ms","start":"2025-05-06T09:41:01.634343Z","end":"2025-05-06T09:41:01.860456Z","steps":["trace[1365702652] 'agreement among raft nodes before linearized reading'  (duration: 225.863903ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:15.256819Z","caller":"traceutil/trace.go:171","msg":"trace[1397901336] transaction","detail":"{read_only:false; response_revision:624; number_of_response:1; }","duration":"157.975912ms","start":"2025-05-06T09:41:15.098792Z","end":"2025-05-06T09:41:15.256768Z","steps":["trace[1397901336] 'process raft request'  (duration: 157.618596ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:15.271630Z","caller":"traceutil/trace.go:171","msg":"trace[1367436799] linearizableReadLoop","detail":"{readStateIndex:682; appliedIndex:680; }","duration":"138.07608ms","start":"2025-05-06T09:41:15.133502Z","end":"2025-05-06T09:41:15.271578Z","steps":["trace[1367436799] 'read index received'  (duration: 122.851617ms)","trace[1367436799] 'applied index is now lower than readState.Index'  (duration: 15.221713ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T09:41:15.272046Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.615356ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:41:15.272244Z","caller":"traceutil/trace.go:171","msg":"trace[2017972552] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:625; }","duration":"138.930231ms","start":"2025-05-06T09:41:15.133277Z","end":"2025-05-06T09:41:15.272208Z","steps":["trace[2017972552] 'agreement among raft nodes before linearized reading'  (duration: 138.521765ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:15.273466Z","caller":"traceutil/trace.go:171","msg":"trace[1656910244] transaction","detail":"{read_only:false; response_revision:625; number_of_response:1; }","duration":"157.683771ms","start":"2025-05-06T09:41:15.115736Z","end":"2025-05-06T09:41:15.273420Z","steps":["trace[1656910244] 'process raft request'  (duration: 155.414562ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:22.389790Z","caller":"traceutil/trace.go:171","msg":"trace[91788224] transaction","detail":"{read_only:false; response_revision:630; number_of_response:1; }","duration":"139.888607ms","start":"2025-05-06T09:41:22.249013Z","end":"2025-05-06T09:41:22.388902Z","steps":["trace[91788224] 'process raft request'  (duration: 138.389031ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:41:24.549133Z","caller":"traceutil/trace.go:171","msg":"trace[542426814] transaction","detail":"{read_only:false; response_revision:631; number_of_response:1; }","duration":"111.54563ms","start":"2025-05-06T09:41:24.437537Z","end":"2025-05-06T09:41:24.549083Z","steps":["trace[542426814] 'process raft request'  (duration: 109.958971ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:42:40.190487Z","caller":"traceutil/trace.go:171","msg":"trace[548955621] transaction","detail":"{read_only:false; response_revision:689; number_of_response:1; }","duration":"325.479662ms","start":"2025-05-06T09:42:39.864969Z","end":"2025-05-06T09:42:40.190449Z","steps":["trace[548955621] 'process raft request'  (duration: 325.195587ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T09:42:40.191255Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T09:42:39.864788Z","time spent":"325.946796ms","remote":"127.0.0.1:40818","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:688 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-05-06T09:42:43.562376Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.614559ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T09:42:43.562714Z","caller":"traceutil/trace.go:171","msg":"trace[2054066450] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:693; }","duration":"124.085818ms","start":"2025-05-06T09:42:43.438579Z","end":"2025-05-06T09:42:43.562665Z","steps":["trace[2054066450] 'range keys from in-memory index tree'  (duration: 123.401159ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T09:44:20.903851Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-06T09:44:20.912439Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-05-06T09:44:20.912932Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-06T09:44:20.913126Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2025/05/06 09:44:20 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-06T09:44:21.029841Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-06T09:44:21.029912Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-06T09:44:21.031677Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-06T09:44:21.056060Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-06T09:44:21.057113Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-06T09:44:21.057166Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [f08374ee2e01] <==
{"level":"info","ts":"2025-05-06T10:53:07.585690Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1129}
{"level":"warn","ts":"2025-05-06T10:53:08.533350Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"503.03907ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037063719040156 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1387 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-06T10:53:08.545009Z","caller":"traceutil/trace.go:171","msg":"trace[706569683] transaction","detail":"{read_only:false; response_revision:1389; number_of_response:1; }","duration":"541.63602ms","start":"2025-05-06T10:53:07.994619Z","end":"2025-05-06T10:53:08.536255Z","steps":["trace[706569683] 'process raft request'  (duration: 18.70704ms)","trace[706569683] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 501.48963ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T10:53:08.545417Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T10:53:07.994597Z","time spent":"550.62018ms","remote":"127.0.0.1:60380","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1387 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-05-06T10:53:08.654899Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1129,"took":"1.05261597s","hash":4191637739,"current-db-size-bytes":3112960,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1417216,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-05-06T10:53:08.657778Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4191637739,"revision":1129,"compact-revision":-1}
2025/05/06 10:53:21 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-06T10:53:22.144899Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"770.10174ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037063719040199 > lease_revoke:<id:70cc96a5302eb482>","response":"size:29"}
{"level":"warn","ts":"2025-05-06T10:53:22.199066Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.13895855s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:53:22.207327Z","caller":"traceutil/trace.go:171","msg":"trace[1926559931] linearizableReadLoop","detail":"{readStateIndex:1628; appliedIndex:1627; }","duration":"1.12823046s","start":"2025-05-06T10:53:21.054848Z","end":"2025-05-06T10:53:22.183079Z","steps":["trace[1926559931] 'read index received'  (duration: 286.35624ms)","trace[1926559931] 'applied index is now lower than readState.Index'  (duration: 841.87125ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T10:53:22.212903Z","caller":"traceutil/trace.go:171","msg":"trace[1888735191] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1395; }","duration":"1.15182036s","start":"2025-05-06T10:53:21.054820Z","end":"2025-05-06T10:53:22.206640Z","steps":["trace[1888735191] 'agreement among raft nodes before linearized reading'  (duration: 1.12840119s)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:53:22.272246Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.15555815s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-05-06T10:53:22.272701Z","caller":"traceutil/trace.go:171","msg":"trace[359693671] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1395; }","duration":"1.15606746s","start":"2025-05-06T10:53:21.116564Z","end":"2025-05-06T10:53:22.272631Z","steps":["trace[359693671] 'agreement among raft nodes before linearized reading'  (duration: 1.10539881s)","trace[359693671] 'range keys from bolt db'  (duration: 30.07773ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T10:53:22.288769Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T10:53:21.116518Z","time spent":"1.17207927s","remote":"127.0.0.1:60380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-06T10:53:48.755363Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"156.31767ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-06T10:53:48.756136Z","caller":"traceutil/trace.go:171","msg":"trace[250036898] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:1437; }","duration":"157.05162ms","start":"2025-05-06T10:53:48.598746Z","end":"2025-05-06T10:53:48.755797Z","steps":["trace[250036898] 'count revisions from in-memory index tree'  (duration: 155.87415ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:18.092437Z","caller":"traceutil/trace.go:171","msg":"trace[634534645] transaction","detail":"{read_only:false; response_revision:1461; number_of_response:1; }","duration":"149.1516ms","start":"2025-05-06T10:54:17.943217Z","end":"2025-05-06T10:54:18.092369Z","steps":["trace[634534645] 'process raft request'  (duration: 148.50459ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:23.334180Z","caller":"traceutil/trace.go:171","msg":"trace[1451729753] transaction","detail":"{read_only:false; response_revision:1468; number_of_response:1; }","duration":"102.03147ms","start":"2025-05-06T10:54:23.232100Z","end":"2025-05-06T10:54:23.334131Z","steps":["trace[1451729753] 'process raft request'  (duration: 100.55097ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:27.765275Z","caller":"traceutil/trace.go:171","msg":"trace[1761129856] transaction","detail":"{read_only:false; response_revision:1471; number_of_response:1; }","duration":"156.75606ms","start":"2025-05-06T10:54:27.608227Z","end":"2025-05-06T10:54:27.764983Z","steps":["trace[1761129856] 'process raft request'  (duration: 155.95992ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:54:27.765337Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.92547ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:54:27.764841Z","caller":"traceutil/trace.go:171","msg":"trace[860198512] linearizableReadLoop","detail":"{readStateIndex:1718; appliedIndex:1717; }","duration":"135.30132ms","start":"2025-05-06T10:54:27.629356Z","end":"2025-05-06T10:54:27.764658Z","steps":["trace[860198512] 'read index received'  (duration: 134.50653ms)","trace[860198512] 'applied index is now lower than readState.Index'  (duration: 793.08¬µs)"],"step_count":2}
{"level":"info","ts":"2025-05-06T10:54:27.766969Z","caller":"traceutil/trace.go:171","msg":"trace[1198797287] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1471; }","duration":"137.4012ms","start":"2025-05-06T10:54:27.629351Z","end":"2025-05-06T10:54:27.766752Z","steps":["trace[1198797287] 'agreement among raft nodes before linearized reading'  (duration: 135.81558ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:54:27.879110Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.31528ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:54:27.879647Z","caller":"traceutil/trace.go:171","msg":"trace[397125628] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:1471; }","duration":"117.10458ms","start":"2025-05-06T10:54:27.762487Z","end":"2025-05-06T10:54:27.879591Z","steps":["trace[397125628] 'agreement among raft nodes before linearized reading'  (duration: 102.13569ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:54:27.879264Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.84038ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:54:27.880100Z","caller":"traceutil/trace.go:171","msg":"trace[1583540115] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1471; }","duration":"185.76711ms","start":"2025-05-06T10:54:27.694261Z","end":"2025-05-06T10:54:27.880028Z","steps":["trace[1583540115] 'agreement among raft nodes before linearized reading'  (duration: 170.3052ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:31.502051Z","caller":"traceutil/trace.go:171","msg":"trace[1955558247] transaction","detail":"{read_only:false; response_revision:1476; number_of_response:1; }","duration":"215.03079ms","start":"2025-05-06T10:54:31.286955Z","end":"2025-05-06T10:54:31.501986Z","steps":["trace[1955558247] 'process raft request'  (duration: 211.71942ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:40.713674Z","caller":"traceutil/trace.go:171","msg":"trace[50063314] transaction","detail":"{read_only:false; response_revision:1485; number_of_response:1; }","duration":"140.2722ms","start":"2025-05-06T10:54:40.573370Z","end":"2025-05-06T10:54:40.713643Z","steps":["trace[50063314] 'process raft request'  (duration: 139.93722ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:54:51.943342Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"244.60803ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-06T10:54:51.943411Z","caller":"traceutil/trace.go:171","msg":"trace[1299039785] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:1492; }","duration":"244.71477ms","start":"2025-05-06T10:54:51.698680Z","end":"2025-05-06T10:54:51.943395Z","steps":["trace[1299039785] 'count revisions from in-memory index tree'  (duration: 244.46457ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:54:58.979771Z","caller":"traceutil/trace.go:171","msg":"trace[484042804] transaction","detail":"{read_only:false; response_revision:1501; number_of_response:1; }","duration":"110.57544ms","start":"2025-05-06T10:54:58.864935Z","end":"2025-05-06T10:54:58.975510Z","steps":["trace[484042804] 'process raft request'  (duration: 109.50894ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:54:59.358403Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.85563ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:54:59.358578Z","caller":"traceutil/trace.go:171","msg":"trace[635958156] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1501; }","duration":"145.0404ms","start":"2025-05-06T10:54:59.213496Z","end":"2025-05-06T10:54:59.358536Z","steps":["trace[635958156] 'range keys from in-memory index tree'  (duration: 144.82926ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:55:07.397161Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.38498ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:55:07.397257Z","caller":"traceutil/trace.go:171","msg":"trace[1064581530] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1509; }","duration":"184.50495ms","start":"2025-05-06T10:55:07.212730Z","end":"2025-05-06T10:55:07.397235Z","steps":["trace[1064581530] 'range keys from in-memory index tree'  (duration: 184.36356ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:55:07.397860Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"138.01086ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-06T10:55:07.397934Z","caller":"traceutil/trace.go:171","msg":"trace[614320952] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1509; }","duration":"138.22812ms","start":"2025-05-06T10:55:07.259683Z","end":"2025-05-06T10:55:07.397911Z","steps":["trace[614320952] 'range keys from in-memory index tree'  (duration: 137.70153ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:58:09.472799Z","caller":"traceutil/trace.go:171","msg":"trace[930162568] transaction","detail":"{read_only:false; response_revision:1671; number_of_response:1; }","duration":"178.20315ms","start":"2025-05-06T10:58:09.294574Z","end":"2025-05-06T10:58:09.472777Z","steps":["trace[930162568] 'process raft request'  (duration: 177.99579ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:58:12.866207Z","caller":"traceutil/trace.go:171","msg":"trace[1580924880] transaction","detail":"{read_only:false; response_revision:1674; number_of_response:1; }","duration":"114.43284ms","start":"2025-05-06T10:58:12.751706Z","end":"2025-05-06T10:58:12.866139Z","steps":["trace[1580924880] 'process raft request'  (duration: 114.02064ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:58:20.176371Z","caller":"traceutil/trace.go:171","msg":"trace[1153708452] transaction","detail":"{read_only:false; response_revision:1681; number_of_response:1; }","duration":"450.75339ms","start":"2025-05-06T10:58:19.725559Z","end":"2025-05-06T10:58:20.176312Z","steps":["trace[1153708452] 'process raft request'  (duration: 448.63893ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:58:20.176831Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T10:58:19.725509Z","time spent":"451.04589ms","remote":"127.0.0.1:60500","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1671 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-05-06T10:58:39.984489Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1388}
{"level":"info","ts":"2025-05-06T10:58:40.038564Z","caller":"traceutil/trace.go:171","msg":"trace[994199017] compact","detail":"{revision:1388; response_revision:1696; }","duration":"108.17739ms","start":"2025-05-06T10:58:39.876313Z","end":"2025-05-06T10:58:39.984490Z","steps":["trace[994199017] 'process raft request'  (duration: 42.25023ms)","trace[994199017] 'check and update compact revision'  (duration: 65.86182ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T10:58:40.179108Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1388,"took":"194.23143ms","hash":1154378746,"current-db-size-bytes":3112960,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1843200,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-06T10:58:40.179193Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1154378746,"revision":1388,"compact-revision":1129}
{"level":"info","ts":"2025-05-06T10:59:47.665422Z","caller":"traceutil/trace.go:171","msg":"trace[203529859] linearizableReadLoop","detail":"{readStateIndex:2082; appliedIndex:2081; }","duration":"119.38086ms","start":"2025-05-06T10:59:47.545977Z","end":"2025-05-06T10:59:47.665358Z","steps":["trace[203529859] 'read index received'  (duration: 110.15001ms)","trace[203529859] 'applied index is now lower than readState.Index'  (duration: 9.22707ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T10:59:47.666103Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.08556ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/website-deployment-77946c4894-sjttb\" limit:1 ","response":"range_response_count:1 size:1864"}
{"level":"info","ts":"2025-05-06T10:59:47.666254Z","caller":"traceutil/trace.go:171","msg":"trace[672445107] range","detail":"{range_begin:/registry/pods/default/website-deployment-77946c4894-sjttb; range_end:; response_count:1; response_revision:1771; }","duration":"120.33342ms","start":"2025-05-06T10:59:47.545878Z","end":"2025-05-06T10:59:47.666211Z","steps":["trace[672445107] 'agreement among raft nodes before linearized reading'  (duration: 120.05379ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:59:47.682421Z","caller":"traceutil/trace.go:171","msg":"trace[476121697] transaction","detail":"{read_only:false; response_revision:1773; number_of_response:1; }","duration":"127.74753ms","start":"2025-05-06T10:59:47.554624Z","end":"2025-05-06T10:59:47.682372Z","steps":["trace[476121697] 'process raft request'  (duration: 127.60641ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:59:47.682530Z","caller":"traceutil/trace.go:171","msg":"trace[567434827] transaction","detail":"{read_only:false; response_revision:1772; number_of_response:1; }","duration":"135.59292ms","start":"2025-05-06T10:59:47.546880Z","end":"2025-05-06T10:59:47.682473Z","steps":["trace[567434827] 'process raft request'  (duration: 135.01305ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:59:47.872924Z","caller":"traceutil/trace.go:171","msg":"trace[293339874] linearizableReadLoop","detail":"{readStateIndex:2086; appliedIndex:2085; }","duration":"103.45788ms","start":"2025-05-06T10:59:47.769427Z","end":"2025-05-06T10:59:47.872885Z","steps":["trace[293339874] 'read index received'  (duration: 12.47139ms)","trace[293339874] 'applied index is now lower than readState.Index'  (duration: 90.98442ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-06T10:59:47.873581Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.11425ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-05-06T10:59:47.873807Z","caller":"traceutil/trace.go:171","msg":"trace[779930477] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:1775; }","duration":"104.38137ms","start":"2025-05-06T10:59:47.769312Z","end":"2025-05-06T10:59:47.873693Z","steps":["trace[779930477] 'agreement among raft nodes before linearized reading'  (duration: 103.84443ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:59:47.874103Z","caller":"traceutil/trace.go:171","msg":"trace[93531594] transaction","detail":"{read_only:false; response_revision:1775; number_of_response:1; }","duration":"181.66122ms","start":"2025-05-06T10:59:47.692347Z","end":"2025-05-06T10:59:47.874008Z","steps":["trace[93531594] 'process raft request'  (duration: 89.59392ms)","trace[93531594] 'compare'  (duration: 90.66663ms)"],"step_count":2}
{"level":"info","ts":"2025-05-06T10:59:52.771769Z","caller":"traceutil/trace.go:171","msg":"trace[1503736918] transaction","detail":"{read_only:false; response_revision:1786; number_of_response:1; }","duration":"323.0721ms","start":"2025-05-06T10:59:52.448639Z","end":"2025-05-06T10:59:52.771711Z","steps":["trace[1503736918] 'process raft request'  (duration: 322.52958ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-06T10:59:52.772257Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-06T10:59:52.448601Z","time spent":"323.35227ms","remote":"127.0.0.1:60500","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1745 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-05-06T10:59:53.125700Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"243.47178ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-05-06T10:59:53.125766Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.32781ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-05-06T10:59:53.125876Z","caller":"traceutil/trace.go:171","msg":"trace[1463697669] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:1786; }","duration":"132.5007ms","start":"2025-05-06T10:59:52.993351Z","end":"2025-05-06T10:59:53.125852Z","steps":["trace[1463697669] 'range keys from in-memory index tree'  (duration: 131.65497ms)"],"step_count":1}
{"level":"info","ts":"2025-05-06T10:59:53.125877Z","caller":"traceutil/trace.go:171","msg":"trace[767333003] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1786; }","duration":"243.71091ms","start":"2025-05-06T10:59:52.882126Z","end":"2025-05-06T10:59:53.125837Z","steps":["trace[767333003] 'range keys from in-memory index tree'  (duration: 243.44019ms)"],"step_count":1}


==> kernel <==
 11:01:17 up  3:35,  0 users,  load average: 2.18, 1.34, 1.07
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [35672d8a15ae] <==
I0506 09:44:21.004891       1 secure_serving.go:258] Stopped listening on [::]:8443
I0506 09:44:21.004937       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0506 09:44:21.005679       1 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0506 09:44:21.005862       1 dynamic_serving_content.go:149] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0506 09:44:21.006572       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0506 09:44:21.007127       1 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
W0506 09:44:21.939381       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939510       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939595       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939676       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939682       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939756       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939863       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.939953       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.940300       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.941693       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.941846       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951534       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951558       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951605       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951690       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951722       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951734       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951789       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951823       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951833       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951878       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951922       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951966       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951975       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952015       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952058       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952113       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952163       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952205       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952220       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952256       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952295       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952313       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952346       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951534       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952383       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952058       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952169       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952467       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952472       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952552       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952561       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.951922       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952115       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952660       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952606       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952706       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952652       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952750       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952552       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952801       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952853       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.952875       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0506 09:44:21.994585       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [48649d962f2c] <==
I0506 10:42:07.813219       1 secure_serving.go:213] Serving securely on [::]:8443
I0506 10:42:07.813625       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0506 10:42:07.813668       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0506 10:42:07.813796       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0506 10:42:07.814036       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0506 10:42:07.814141       1 aggregator.go:169] waiting for initial CRD sync...
I0506 10:42:07.814464       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0506 10:42:07.815263       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0506 10:42:07.835929       1 controller.go:119] Starting legacy_token_tracking_controller
I0506 10:42:07.835996       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0506 10:42:07.840690       1 local_available_controller.go:156] Starting LocalAvailability controller
I0506 10:42:07.840731       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0506 10:42:07.867978       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0506 10:42:07.868111       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0506 10:42:07.868219       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0506 10:42:07.815259       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0506 10:42:07.868279       1 controller.go:78] Starting OpenAPI AggregationController
I0506 10:42:07.870226       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0506 10:42:07.870332       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0506 10:42:07.870625       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0506 10:42:07.870672       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0506 10:42:07.870721       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0506 10:42:07.870793       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0506 10:42:07.870922       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0506 10:42:07.870964       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0506 10:42:07.871442       1 controller.go:142] Starting OpenAPI controller
I0506 10:42:07.871532       1 controller.go:90] Starting OpenAPI V3 controller
I0506 10:42:07.871657       1 naming_controller.go:294] Starting NamingConditionController
I0506 10:42:07.871819       1 establishing_controller.go:81] Starting EstablishingController
I0506 10:42:07.872031       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0506 10:42:07.872117       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0506 10:42:07.872140       1 crd_finalizer.go:269] Starting CRDFinalizer
I0506 10:42:07.874083       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0506 10:42:07.874264       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0506 10:42:08.090225       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0506 10:42:08.090258       1 policy_source.go:240] refreshing policies
I0506 10:42:08.103292       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0506 10:42:08.113940       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0506 10:42:08.114155       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0506 10:42:08.172985       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0506 10:42:08.173055       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0506 10:42:08.173208       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0506 10:42:08.173276       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0506 10:42:08.173288       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0506 10:42:08.173612       1 aggregator.go:171] initial CRD sync complete...
I0506 10:42:08.173647       1 autoregister_controller.go:144] Starting autoregister controller
I0506 10:42:08.173660       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0506 10:42:08.173671       1 cache.go:39] Caches are synced for autoregister controller
I0506 10:42:08.174585       1 shared_informer.go:320] Caches are synced for node_authorizer
I0506 10:42:08.174771       1 cache.go:39] Caches are synced for LocalAvailability controller
I0506 10:42:08.174906       1 shared_informer.go:320] Caches are synced for configmaps
I0506 10:42:08.875015       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0506 10:42:09.409624       1 controller.go:615] quota admission added evaluator for: serviceaccounts
W0506 10:42:12.089044       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0506 10:42:12.093869       1 controller.go:615] quota admission added evaluator for: endpoints
I0506 10:42:15.102908       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0506 10:42:15.192834       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0506 10:42:15.196073       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0506 10:42:15.306401       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0506 10:44:36.538217       1 alloc.go:330] "allocated clusterIPs" service="default/website-service" clusterIPs={"IPv4":"10.108.226.58"}


==> kube-controller-manager [cac9fafba2fb] <==
I0506 09:36:57.399448       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0506 09:36:57.400777       1 shared_informer.go:320] Caches are synced for job
I0506 09:36:57.404165       1 shared_informer.go:320] Caches are synced for stateful set
I0506 09:36:57.491544       1 shared_informer.go:320] Caches are synced for deployment
I0506 09:36:57.515425       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0506 09:36:57.518466       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0506 09:36:57.516793       1 shared_informer.go:320] Caches are synced for endpoint
I0506 09:36:57.517547       1 shared_informer.go:320] Caches are synced for TTL
I0506 09:36:57.591792       1 shared_informer.go:320] Caches are synced for ReplicationController
I0506 09:36:57.518046       1 shared_informer.go:320] Caches are synced for taint
I0506 09:36:57.594728       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0506 09:36:57.595071       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0506 09:36:57.595329       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0506 09:36:57.523017       1 shared_informer.go:320] Caches are synced for resource quota
I0506 09:36:57.523081       1 shared_informer.go:320] Caches are synced for persistent volume
I0506 09:36:57.523119       1 shared_informer.go:320] Caches are synced for daemon sets
I0506 09:36:57.523181       1 shared_informer.go:320] Caches are synced for GC
I0506 09:36:57.575382       1 shared_informer.go:320] Caches are synced for PV protection
I0506 09:36:57.575781       1 shared_informer.go:320] Caches are synced for disruption
I0506 09:36:57.575924       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0506 09:36:57.576006       1 shared_informer.go:320] Caches are synced for attach detach
I0506 09:36:57.576094       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0506 09:36:57.611288       1 shared_informer.go:320] Caches are synced for resource quota
I0506 09:36:57.576134       1 shared_informer.go:320] Caches are synced for node
I0506 09:36:57.612925       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0506 09:36:57.613257       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0506 09:36:57.613327       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0506 09:36:57.613380       1 shared_informer.go:320] Caches are synced for cidrallocator
I0506 09:36:57.576275       1 shared_informer.go:320] Caches are synced for crt configmap
I0506 09:36:57.711171       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0506 09:36:57.875849       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0506 09:36:57.916274       1 shared_informer.go:320] Caches are synced for garbage collector
I0506 09:36:57.992619       1 shared_informer.go:320] Caches are synced for garbage collector
I0506 09:36:57.992728       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0506 09:36:57.992775       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0506 09:36:58.310307       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0506 09:36:58.310685       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:36:58.310808       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:36:58.587605       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:37:02.650098       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.049286145s"
I0506 09:37:02.928253       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="277.935629ms"
I0506 09:37:02.928600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="185.625¬µs"
I0506 09:37:03.470654       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="275.367¬µs"
I0506 09:37:04.126071       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="391.6¬µs"
I0506 09:37:04.442276       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:37:11.427374       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:37:14.022285       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="391.097434ms"
I0506 09:37:14.212722       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="190.262116ms"
I0506 09:37:14.213177       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="221.191¬µs"
I0506 09:37:16.912469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="117.334¬µs"
I0506 09:37:18.342658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="300.85¬µs"
I0506 09:37:27.919028       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="142.909¬µs"
I0506 09:37:28.988192       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="86.9¬µs"
I0506 09:37:29.015751       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="229.35¬µs"
I0506 09:37:52.544544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="30.675161ms"
I0506 09:37:52.544685       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="76.083¬µs"
I0506 09:40:20.568154       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:41:27.356910       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:42:01.156177       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 09:44:04.999774       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [dce153cffada] <==
I0506 10:44:12.173771       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 10:44:35.932762       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="96.865093ms"
I0506 10:44:35.962644       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="29.753628ms"
I0506 10:44:35.963132       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="85.525¬µs"
I0506 10:44:35.963343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="73.517¬µs"
I0506 10:44:35.994437       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="52.617¬µs"
I0506 10:44:41.601164       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="136.583¬µs"
I0506 10:44:47.712576       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="63.433¬µs"
I0506 10:45:20.308999       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="82.5¬µs"
I0506 10:45:32.295719       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="52.984¬µs"
I0506 10:45:47.309008       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="285.451¬µs"
I0506 10:46:01.177819       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="262.412¬µs"
I0506 10:46:49.982809       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="50.13¬µs"
I0506 10:47:02.013211       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="74.43¬µs"
I0506 10:48:25.532610       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="79.682¬µs"
I0506 10:48:36.590072       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="584.01¬µs"
I0506 10:51:19.140731       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 10:51:40.082781       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="104.49¬µs"
I0506 10:51:53.533376       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 10:51:56.637479       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="162.81¬µs"
I0506 10:53:40.055897       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="294.20244ms"
I0506 10:53:40.119773       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="57.45105ms"
I0506 10:53:40.122146       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="41.4¬µs"
I0506 10:53:40.188659       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="94.05¬µs"
I0506 10:53:40.325370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="65.88¬µs"
I0506 10:53:46.983530       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="47.52¬µs"
I0506 10:53:59.406535       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="122.22¬µs"
I0506 10:54:06.956336       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 10:54:18.163746       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="5.05386ms"
I0506 10:54:31.032856       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="77.67¬µs"
I0506 10:54:31.521670       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 10:54:51.975818       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="62.64¬µs"
I0506 10:55:06.662089       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="136.26¬µs"
I0506 10:55:40.156121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="220.32¬µs"
I0506 10:55:55.164023       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="163.89¬µs"
I0506 10:57:16.912997       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="189.9¬µs"
I0506 10:57:19.892846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="132.48¬µs"
I0506 10:57:30.915643       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="347.76¬µs"
I0506 10:57:31.917105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="153.81¬µs"
I0506 10:59:47.251370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="10.03977ms"
I0506 10:59:47.251647       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="68.49¬µs"
I0506 10:59:47.374286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="95.55633ms"
I0506 10:59:47.406954       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="32.60898ms"
I0506 10:59:47.407152       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="89.91¬µs"
I0506 10:59:47.454290       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="404.64¬µs"
I0506 10:59:47.481983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="135.34182ms"
I0506 10:59:47.694958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="212.80212ms"
I0506 10:59:47.695296       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="185.22¬µs"
I0506 10:59:47.877551       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="166.59¬µs"
I0506 10:59:48.650909       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="97.02¬µs"
I0506 10:59:48.961675       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="262.71¬µs"
I0506 10:59:49.005141       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="54.36¬µs"
I0506 10:59:49.095539       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-f59457478" duration="152.82¬µs"
I0506 10:59:57.991981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="105.3¬µs"
I0506 10:59:58.635606       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0506 11:00:15.845665       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="61.65¬µs"
I0506 11:00:32.804266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="291.42¬µs"
I0506 11:00:38.838094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="511.56¬µs"
I0506 11:00:52.404134       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-7666cd884b" duration="159.93¬µs"
I0506 11:00:56.423910       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/website-deployment-77946c4894" duration="49.05¬µs"


==> kube-proxy [b593fb53167c] <==
I0506 09:37:16.595894       1 server_linux.go:66] "Using iptables proxy"
I0506 09:37:17.273371       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0506 09:37:17.273665       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0506 09:37:17.373539       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0506 09:37:17.373638       1 server_linux.go:170] "Using iptables Proxier"
I0506 09:37:17.430887       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0506 09:37:17.474229       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0506 09:37:17.509515       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0506 09:37:17.509653       1 server.go:497] "Version info" version="v1.32.0"
I0506 09:37:17.509741       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0506 09:37:17.517851       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0506 09:37:17.524050       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0506 09:37:17.572220       1 config.go:105] "Starting endpoint slice config controller"
I0506 09:37:17.572954       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0506 09:37:17.574722       1 config.go:199] "Starting service config controller"
I0506 09:37:17.582042       1 shared_informer.go:313] Waiting for caches to sync for service config
I0506 09:37:17.582256       1 config.go:329] "Starting node config controller"
I0506 09:37:17.582298       1 shared_informer.go:313] Waiting for caches to sync for node config
I0506 09:37:17.675198       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0506 09:37:17.710110       1 shared_informer.go:320] Caches are synced for node config
I0506 09:37:17.710381       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [eb0817294c16] <==
I0506 10:42:21.117226       1 server_linux.go:66] "Using iptables proxy"
I0506 10:42:26.213398       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0506 10:42:26.225349       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0506 10:42:26.369601       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0506 10:42:26.369800       1 server_linux.go:170] "Using iptables Proxier"
I0506 10:42:26.386288       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0506 10:42:26.404555       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0506 10:42:26.426107       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0506 10:42:26.427840       1 server.go:497] "Version info" version="v1.32.0"
I0506 10:42:26.427953       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0506 10:42:26.494603       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0506 10:42:26.522841       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0506 10:42:26.554547       1 config.go:105] "Starting endpoint slice config controller"
I0506 10:42:26.554604       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0506 10:42:26.554806       1 config.go:199] "Starting service config controller"
I0506 10:42:26.554832       1 shared_informer.go:313] Waiting for caches to sync for service config
I0506 10:42:26.578040       1 config.go:329] "Starting node config controller"
I0506 10:42:26.578143       1 shared_informer.go:313] Waiting for caches to sync for node config
I0506 10:42:26.660218       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0506 10:42:26.661973       1 shared_informer.go:320] Caches are synced for service config
I0506 10:42:26.679265       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [c55e16abc80f] <==
I0506 10:42:02.114603       1 serving.go:386] Generated self-signed cert in-memory
W0506 10:42:08.082952       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0506 10:42:08.083075       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0506 10:42:08.083147       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0506 10:42:08.083163       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0506 10:42:08.220683       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0506 10:42:08.220715       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0506 10:42:08.236176       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0506 10:42:08.236833       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0506 10:42:08.237034       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0506 10:42:08.237398       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0506 10:42:08.368203       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [e75ff50645b1] <==
W0506 09:36:47.098952       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0506 09:36:47.099612       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.174910       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0506 09:36:47.175059       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.195644       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0506 09:36:47.195760       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.238045       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0506 09:36:47.238232       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.254770       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:47.254950       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.268033       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0506 09:36:47.268215       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.384547       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0506 09:36:47.386682       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.407515       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0506 09:36:47.407671       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.480825       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0506 09:36:47.480917       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.579156       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0506 09:36:47.579323       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.605449       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:47.605624       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.682254       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:47.682399       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:47.682719       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0506 09:36:47.682922       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0506 09:36:48.747108       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0506 09:36:48.747323       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:48.903204       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:48.903412       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.225293       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0506 09:36:49.225384       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.331401       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0506 09:36:49.331546       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.399272       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:49.399488       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.432401       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:49.432977       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.432729       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0506 09:36:49.433177       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.536066       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0506 09:36:49.536223       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.706342       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:49.706570       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.750204       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0506 09:36:49.750439       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.928161       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0506 09:36:49.928457       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0506 09:36:49.980081       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0506 09:36:49.980350       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:50.031313       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0506 09:36:50.031590       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0506 09:36:50.060535       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0506 09:36:50.060775       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0506 09:36:50.529246       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0506 09:36:50.529392       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0506 09:36:50.580075       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0506 09:36:50.580281       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
I0506 09:36:54.581540       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0506 09:44:20.911668       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
May 06 10:56:45 minikube kubelet[1582]: E0506 10:56:45.256464    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:57:00 minikube kubelet[1582]: E0506 10:57:00.145470    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 10:57:00 minikube kubelet[1582]: E0506 10:57:00.145774    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 10:57:00 minikube kubelet[1582]: E0506 10:57:00.146250    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website,Image:my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2448,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-7666cd884b-hpnc7_default(dabcea3e-5d04-4e61-b0b0-74458a81940a): ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 06 10:57:00 minikube kubelet[1582]: E0506 10:57:00.147633    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ErrImagePull: \"Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:57:03 minikube kubelet[1582]: E0506 10:57:03.036901    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 10:57:03 minikube kubelet[1582]: E0506 10:57:03.039757    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 10:57:03 minikube kubelet[1582]: E0506 10:57:03.040354    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website,Image:my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zk77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-f59457478-cqjsc_default(3b352288-019f-41e0-b744-835925687af8): ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 06 10:57:03 minikube kubelet[1582]: E0506 10:57:03.042339    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ErrImagePull: \"Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:57:16 minikube kubelet[1582]: E0506 10:57:16.843368    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:57:19 minikube kubelet[1582]: E0506 10:57:19.843446    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:57:30 minikube kubelet[1582]: E0506 10:57:30.845430    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:57:31 minikube kubelet[1582]: E0506 10:57:31.846314    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:57:45 minikube kubelet[1582]: E0506 10:57:45.844601    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:57:45 minikube kubelet[1582]: E0506 10:57:45.845053    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:58:00 minikube kubelet[1582]: E0506 10:58:00.424887    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:58:00 minikube kubelet[1582]: E0506 10:58:00.425019    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:58:11 minikube kubelet[1582]: E0506 10:58:11.423617    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:58:15 minikube kubelet[1582]: E0506 10:58:15.423030    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:58:30 minikube kubelet[1582]: E0506 10:58:30.007204    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:58:32 minikube kubelet[1582]: E0506 10:58:32.006779    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:58:44 minikube kubelet[1582]: E0506 10:58:44.008990    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:58:45 minikube kubelet[1582]: E0506 10:58:45.032343    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:58:56 minikube kubelet[1582]: E0506 10:58:56.009755    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:58:58 minikube kubelet[1582]: E0506 10:58:58.007346    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:59:14 minikube kubelet[1582]: E0506 10:59:14.589624    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:59:16 minikube kubelet[1582]: E0506 10:59:16.589763    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:59:28 minikube kubelet[1582]: E0506 10:59:28.587774    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:59:28 minikube kubelet[1582]: E0506 10:59:28.587877    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:59:43 minikube kubelet[1582]: E0506 10:59:43.182657    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:59:46 minikube kubelet[1582]: E0506 10:59:46.177486    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-f59457478-cqjsc" podUID="3b352288-019f-41e0-b744-835925687af8"
May 06 10:59:47 minikube kubelet[1582]: I0506 10:59:47.579398    1582 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5bw7w\" (UniqueName: \"kubernetes.io/projected/caa0e51e-4861-4cc9-92d9-8f8b337e6d50-kube-api-access-5bw7w\") pod \"website-deployment-77946c4894-sjttb\" (UID: \"caa0e51e-4861-4cc9-92d9-8f8b337e6d50\") " pod="default/website-deployment-77946c4894-sjttb"
May 06 10:59:48 minikube kubelet[1582]: I0506 10:59:48.757188    1582 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-4zk77\" (UniqueName: \"kubernetes.io/projected/3b352288-019f-41e0-b744-835925687af8-kube-api-access-4zk77\") pod \"3b352288-019f-41e0-b744-835925687af8\" (UID: \"3b352288-019f-41e0-b744-835925687af8\") "
May 06 10:59:48 minikube kubelet[1582]: I0506 10:59:48.788648    1582 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3b352288-019f-41e0-b744-835925687af8-kube-api-access-4zk77" (OuterVolumeSpecName: "kube-api-access-4zk77") pod "3b352288-019f-41e0-b744-835925687af8" (UID: "3b352288-019f-41e0-b744-835925687af8"). InnerVolumeSpecName "kube-api-access-4zk77". PluginName "kubernetes.io/projected", VolumeGIDValue ""
May 06 10:59:48 minikube kubelet[1582]: I0506 10:59:48.858423    1582 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-4zk77\" (UniqueName: \"kubernetes.io/projected/3b352288-019f-41e0-b744-835925687af8-kube-api-access-4zk77\") on node \"minikube\" DevicePath \"\""
May 06 10:59:49 minikube kubelet[1582]: I0506 10:59:49.704688    1582 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8dd67aa73f6f997fb8907e18c3dacbeb315e74bd94b9dbc12b4a5d87aba37bd4"
May 06 10:59:50 minikube kubelet[1582]: I0506 10:59:50.224952    1582 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="3b352288-019f-41e0-b744-835925687af8" path="/var/lib/kubelet/pods/3b352288-019f-41e0-b744-835925687af8/volumes"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.177700    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.381358    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.381454    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.381629    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website-container,Image:Aryansharma2206/my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-77946c4894-sjttb_default(caa0e51e-4861-4cc9-92d9-8f8b337e6d50): ErrImagePull: Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" logger="UnhandledError"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.383311    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website-container\" with ErrImagePull: \"Error response from daemon: Get \\\"https://Aryansharma2206/v2/\\\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host\"" pod="default/website-deployment-77946c4894-sjttb" podUID="caa0e51e-4861-4cc9-92d9-8f8b337e6d50"
May 06 10:59:57 minikube kubelet[1582]: E0506 10:59:57.939715    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website-container\" with ImagePullBackOff: \"Back-off pulling image \\\"Aryansharma2206/my-website-image\\\": ErrImagePull: Error response from daemon: Get \\\"https://Aryansharma2206/v2/\\\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host\"" pod="default/website-deployment-77946c4894-sjttb" podUID="caa0e51e-4861-4cc9-92d9-8f8b337e6d50"
May 06 11:00:18 minikube kubelet[1582]: E0506 11:00:18.202286    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 11:00:18 minikube kubelet[1582]: E0506 11:00:18.202417    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-website-image:latest"
May 06 11:00:18 minikube kubelet[1582]: E0506 11:00:18.202685    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website,Image:my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2448,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-7666cd884b-hpnc7_default(dabcea3e-5d04-4e61-b0b0-74458a81940a): ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 06 11:00:18 minikube kubelet[1582]: E0506 11:00:18.204518    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ErrImagePull: \"Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 11:00:25 minikube kubelet[1582]: E0506 11:00:25.661098    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 11:00:25 minikube kubelet[1582]: E0506 11:00:25.661207    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 11:00:25 minikube kubelet[1582]: E0506 11:00:25.661462    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website-container,Image:Aryansharma2206/my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-77946c4894-sjttb_default(caa0e51e-4861-4cc9-92d9-8f8b337e6d50): ErrImagePull: Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" logger="UnhandledError"
May 06 11:00:25 minikube kubelet[1582]: E0506 11:00:25.662836    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website-container\" with ErrImagePull: \"Error response from daemon: Get \\\"https://Aryansharma2206/v2/\\\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host\"" pod="default/website-deployment-77946c4894-sjttb" podUID="caa0e51e-4861-4cc9-92d9-8f8b337e6d50"
May 06 11:00:32 minikube kubelet[1582]: E0506 11:00:32.748291    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 11:00:38 minikube kubelet[1582]: E0506 11:00:38.747165    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website-container\" with ImagePullBackOff: \"Back-off pulling image \\\"Aryansharma2206/my-website-image\\\": ErrImagePull: Error response from daemon: Get \\\"https://Aryansharma2206/v2/\\\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host\"" pod="default/website-deployment-77946c4894-sjttb" podUID="caa0e51e-4861-4cc9-92d9-8f8b337e6d50"
May 06 11:00:52 minikube kubelet[1582]: E0506 11:00:52.351469    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 11:01:03 minikube kubelet[1582]: E0506 11:01:03.867134    1582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 11:01:03 minikube kubelet[1582]: E0506 11:01:03.867442    1582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" image="Aryansharma2206/my-website-image:latest"
May 06 11:01:03 minikube kubelet[1582]: E0506 11:01:03.868071    1582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:website-container,Image:Aryansharma2206/my-website-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bw7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod website-deployment-77946c4894-sjttb_default(caa0e51e-4861-4cc9-92d9-8f8b337e6d50): ErrImagePull: Error response from daemon: Get \"https://Aryansharma2206/v2/\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host" logger="UnhandledError"
May 06 11:01:03 minikube kubelet[1582]: E0506 11:01:03.869781    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website-container\" with ErrImagePull: \"Error response from daemon: Get \\\"https://Aryansharma2206/v2/\\\": dial tcp: lookup Aryansharma2206 on 192.168.49.1:53: no such host\"" pod="default/website-deployment-77946c4894-sjttb" podUID="caa0e51e-4861-4cc9-92d9-8f8b337e6d50"
May 06 11:01:05 minikube kubelet[1582]: E0506 11:01:05.346094    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"
May 06 11:01:16 minikube kubelet[1582]: E0506 11:01:16.346588    1582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"website\" with ImagePullBackOff: \"Back-off pulling image \\\"my-website-image\\\": ErrImagePull: Error response from daemon: pull access denied for my-website-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/website-deployment-7666cd884b-hpnc7" podUID="dabcea3e-5d04-4e61-b0b0-74458a81940a"


==> storage-provisioner [7b5923eda65b] <==
I0506 10:42:18.220955       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0506 10:42:51.195291       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [dbdaf4c278d3] <==
I0506 10:43:07.982100       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0506 10:43:08.090524       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0506 10:43:08.092313       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0506 10:43:25.554628       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0506 10:43:25.555899       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_63ea7c5e-b02c-40f8-97b5-7bd761d3e1f0!
I0506 10:43:25.555796       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"28618490-7a68-44d2-ba68-460840443ea0", APIVersion:"v1", ResourceVersion:"888", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_63ea7c5e-b02c-40f8-97b5-7bd761d3e1f0 became leader
I0506 10:43:25.657180       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_63ea7c5e-b02c-40f8-97b5-7bd761d3e1f0!

